{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Train_distance_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajWavWFE5wkk"
      },
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6zXmXTz6dci"
      },
      "source": [
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation, BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam, Nadam\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback, EarlyStopping\n",
        "from tensorflow import keras\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from wandb.keras import WandbCallback\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "Dataset = namedtuple(\"Dataset\", [\"x\", \"r\", \"fi\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCHPDePw9W7J"
      },
      "source": [
        "# Configure the sweep â€“ specify the parameters to search through, the search strategy, the optimization metric et all.\n",
        "sweep_config = {\n",
        "    'method': 'bayes', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'val_mae',\n",
        "      'goal': 'minimize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'target':{\n",
        "          'values': ['r']  \n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [20]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64, 128]\n",
        "        },\n",
        "        'batchnorm_for_layers':{\n",
        "            'values': [0, 1]\n",
        "        },\n",
        "        'layer_1_size': {\n",
        "            'values': [2048, 3072, 4096, 5040, 5760]\n",
        "        },\n",
        "        'layer_2_size': {\n",
        "            'values': [3072, 4096, 5120, 5760, 6480, 7200]\n",
        "        },\n",
        "        'layer_3_size': {\n",
        "            'values': [2160, 2880, 3600, 4096, 5120, 6144]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'distribution': 'uniform',\n",
        "            'max': 0.001,\n",
        "            'min': 1e-06\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'distribution': 'categorical',\n",
        "            'values': ['sgd', 'rmsprop']\n",
        "        },\n",
        "        'activation1': {\n",
        "            'distribution': 'categorical',\n",
        "            'values': ['relu', 'tanh', 'sigmoid']\n",
        "        },\n",
        "        'activation2': {\n",
        "            'distribution': 'categorical',\n",
        "            'values': ['relu', 'tanh', 'sigmoid']\n",
        "        },\n",
        "        'activation3': {\n",
        "            'distribution': 'categorical',\n",
        "            'values': ['relu', 'tanh', 'sigmoid']\n",
        "        },\n",
        "        'activation4': {\n",
        "            'distribution': 'categorical',\n",
        "            'values': ['relu', 'tanh', 'sigmoid']\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2007xI7CxhN"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"artem-starkov\", project=\"flatfasetgen\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fNJhL5Df9sY"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "def read(data_dir, split):\n",
        "    filename = split + \".npz\"\n",
        "    data = np.load(os.path.join(data_dir, filename))\n",
        "\n",
        "    return Dataset(x=data[\"x\"], r=data[\"r\"], fi=data['fi'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeXHaN2cngr6"
      },
      "source": [
        "# The sweep calls this function with each set of hyperparameters\n",
        "def train():\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'epochs': 20,\n",
        "        'batch_size': 64,\n",
        "        # 'weight_decay': 0.0005,\n",
        "        'learning_rate': 1e-3,\n",
        "        'activation1': 'relu',\n",
        "        'activation2': 'relu',\n",
        "        'activation3': 'relu',\n",
        "        'activation4': 'relu',\n",
        "        'optimizer': 'sgd',\n",
        "        'layer_1_size': 4320,\n",
        "        'layer_2_size': 4320,\n",
        "        'layer_3_size': 4320,\n",
        "        'batchnorm_for_layers': 1,\n",
        "        'Distance_distribution': 'sqr',\n",
        "        'target': 'r'\n",
        "    }\n",
        "\n",
        "    run = wandb.init(project=\"flatfasetgen\", job_type=\"training_distance_linear_distrubution\", config=config_defaults)\n",
        "    processed_data = wandb.Artifact(\"Clear_datasets\", type=\"dataset\")\n",
        "    raw_data_artifact = run.use_artifact('Clear_datasets:v20')  \n",
        "    raw_dataset = raw_data_artifact.download()\n",
        "    train_dataset = read(raw_dataset, 'train_set')\n",
        "    test_dataset = read(raw_dataset, 'test_set')\n",
        "    run.log_artifact(processed_data)\n",
        "    X_train, X_test, y_train, y_test = train_dataset.x, test_dataset.x, train_dataset.r, test_dataset.r\n",
        "\n",
        "    config = wandb.config\n",
        "    model = Sequential()\n",
        "    model.add(Dense(720, input_shape=(720,)))\n",
        "    if config.batchnorm_for_layers:\n",
        "      model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Activation(config.activation1))\n",
        "\n",
        "\n",
        "    model.add(Dense(config.layer_1_size))\n",
        "    if config.batchnorm_for_layers:\n",
        "      model.add(BatchNormalization())\n",
        "    model.add(Activation(config.activation2))\n",
        "\n",
        "    model.add(Dense(config.layer_2_size))\n",
        "    if config.batchnorm_for_layers:\n",
        "      model.add(BatchNormalization())\n",
        "    model.add(Activation(config.activation3))\n",
        "\n",
        "    model.add(Dense(config.layer_3_size))\n",
        "    if config.batchnorm_for_layers:\n",
        "      model.add(BatchNormalization())\n",
        "    model.add(Activation(config.activation4))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Define the optimizer\n",
        "    if config.optimizer=='sgd':\n",
        "      optimizer = SGD(learning_rate=config.learning_rate, decay=1e-5, nesterov=True)\n",
        "    elif config.optimizer=='rmsprop':\n",
        "      optimizer = RMSprop(learning_rate=config.learning_rate, decay=1e-5)\n",
        "\n",
        "    model.compile(loss='mae', optimizer = optimizer, metrics=['mae', 'mape'])\n",
        "\n",
        "    model_artifact = wandb.Artifact(\n",
        "            \"distance_compiled_model\", type=\"model\",\n",
        "            description=f\"50k dataset, full search with batch_norm, desrtibution for distance: {config.Distance_distribution}\",\n",
        "            metadata=dict(config))\n",
        "    model.save(\"distance_compiled_models\")\n",
        "    model_artifact.add_dir(\"distance_compiled_models\")\n",
        "    run.log_artifact(model_artifact)\n",
        "\n",
        "    model.fit(X_train, y_train, batch_size=config.batch_size,\n",
        "              epochs=config.epochs, validation_data=(X_test, y_test),\n",
        "              callbacks=[WandbCallback(validation_data=(X_test, y_test)),\n",
        "                          EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=10, restore_best_weights=True)])\n",
        "    wandb.log({\"R2\" : r2_score(y_test, model.predict(X_test))})\n",
        "    wandb.log({\"RMSE\": mean_squared_error(y_test, model.predict(X_test), squared=False)})\n",
        "    trained_model_artifact = wandb.Artifact(\n",
        "            \"distance_trained_model\", type=\"model\",\n",
        "            description=\"\",\n",
        "            metadata=dict(config))\n",
        "    \n",
        "    model.save('distance_trained_models')\n",
        "    trained_model_artifact.add_dir('distance_trained_models')\n",
        "    run.log_artifact(trained_model_artifact)\n",
        "    run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSJuYpfcI7rj"
      },
      "source": [
        "wandb.agent(sweep_id, train, count=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}